from openai import OpenAI
from core.config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = None
if LLM_API_KEY:
    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

def generate_rag_answer(query: str, retrieved_docs: list):
    """
    RAG ç”Ÿæˆå‡½æ•°ï¼šå‰©èœå¤§æ•‘æ˜Ÿæ¨¡å¼
    """
    if not client:
        return "âŒ é”™è¯¯ï¼šAPI Key æœªé…ç½®ã€‚"

    # 1. å¦‚æœæ²¡æŸ¥åˆ°èœè°±ï¼Œå°è¯•è®© AI ç”¨é€šç”¨çŸ¥è¯†å…œåº•ï¼ˆå› ä¸ºå‰©èœç»„åˆåƒå¥‡ç™¾æ€ªï¼‰
    if not retrieved_docs:
        context_str = "ï¼ˆæ•°æ®åº“ä¸­æœªæ‰¾åˆ°åŒ¹é…èœè°±ï¼Œè¯·åŸºäºä½ çš„é€šç”¨çƒ¹é¥ªçŸ¥è¯†å›ç­”ï¼‰"
    else:
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        context_str = ""
        for i, doc in enumerate(retrieved_docs):
            content_snippet = doc['content'][:600] 
            context_str += f"\nã€å‚è€ƒçµæ„Ÿ {i+1}ã€‘: {doc['name']}\né£Ÿææ ‡ç­¾: {doc['tags']}\nåšæ³•ç‰‡æ®µ: {content_snippet}\n" + "-"*20

    # ==========================================
    # æ ¸å¿ƒä¿®æ”¹ï¼šé’ˆå¯¹â€œå‰©èœå¤„ç†â€çš„ Prompt è®¾è®¡
    # ==========================================
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å……æ»¡åˆ›æ„çš„â€œå†°ç®±æ¸…ç†å¤§å¸ˆâ€å’Œçƒ¹é¥ªä¸“å®¶ã€‚ä½ çš„ç‰¹é•¿æ˜¯æ ¹æ®ç”¨æˆ·æ‰‹é‡Œä»…æœ‰çš„å‡ ç§â€œå‰©èœ/é£Ÿæâ€ï¼Œåˆ©ç”¨å‚è€ƒçµæ„Ÿï¼Œç»™å‡ºä¸€é“å¯è¡Œçš„çƒ¹é¥ªæ–¹æ¡ˆã€‚

    ä½ çš„å›ç­”é€»è¾‘ï¼š
    1. **åˆ†æåŒ¹é…åº¦**ï¼šçœ‹ä¸€çœ¼ã€å‚è€ƒçµæ„Ÿã€‘ï¼Œå‘Šè¯‰ç”¨æˆ·å“ªä¸ªçµæ„Ÿæœ€æ¥è¿‘ä»–çš„é£Ÿæã€‚
    2. **çµæ´»å˜é€š**ï¼šç”¨æˆ·çš„é£Ÿæå¯èƒ½ä¸å…¨ï¼ˆæ¯”å¦‚å‚è€ƒèœè°±è¦é’æ¤’ï¼Œä½†ç”¨æˆ·æ²¡æœ‰ï¼‰ã€‚è¯·æ˜ç¡®å‘Šè¯‰ç”¨æˆ·ï¼šâ€œè™½ç„¶åŸè°±éœ€è¦é’æ¤’ï¼Œä½†ä½ åªç”¨æ‰‹é‡Œçš„è‚‰å’Œè›‹ä¹Ÿå¾ˆå¥½åƒï¼Œåªéœ€æ³¨æ„...â€
    3. **ç®€åŒ–æ­¥éª¤**ï¼šç”¨æˆ·æ˜¯æ¥å¤„ç†å‰©èœçš„ï¼Œé€šå¸¸ä¸æƒ³å¤ªéº»çƒ¦ã€‚è¯·æ€»ç»“å‡ºæœ€æ ¸å¿ƒçš„ 3-4 ä¸ªæ­¥éª¤ã€‚
    4. **é¼“åŠ±å°è¯•**ï¼šè¯­æ°”è¦è½»æ¾ã€å¹½é»˜ï¼Œé¼“åŠ±ç”¨æˆ·å‘æŒ¥åˆ›æ„ã€‚
    """

    user_prompt = f"""
    æˆ‘ç°åœ¨çš„å†°ç®±é‡Œå‰©ä¸‹è¿™äº›é£Ÿæï¼ˆæˆ–è€…æˆ‘æƒ³åƒï¼‰ï¼š
    ã€{query}ã€‘

    è¯·å‚è€ƒä»¥ä¸‹æ•°æ®åº“é‡Œçš„èœè°±çµæ„Ÿï¼Œæ•™æˆ‘æ€ä¹ˆåšï¼š
    {context_str}

    å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„ï¼Œè¯·å¸®æˆ‘é­”æ”¹ä¸€ä¸‹ï¼
    """

    # 4. è°ƒç”¨ API
    try:
        print(f"ğŸ¤– [Generator] å‰©èœæ•‘æ˜Ÿæ­£åœ¨æ€è€ƒ (Model: {LLM_MODEL_NAME})...")
        response = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.8, # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ï¼Œå¢åŠ åˆ›æ„
            max_tokens=1024
        )
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ [Generator] API è°ƒç”¨å¤±è´¥: {e}")
        return "åå¨å¤ªå¿™äº†ï¼Œè¯·ç¨åå†è¯•ï¼"