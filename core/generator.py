from openai import OpenAI
from core.config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL_NAME
import re

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = None
if LLM_API_KEY:
    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

def smart_select_and_comment(query: str, candidates: list):
    """
    æ™ºèƒ½ä¼˜é€‰ Rerank (çµæ´»ç‰ˆ)
    ä¸å†æ­»æ¿è¿‡æ»¤ï¼Œè€Œæ˜¯ä¾§é‡äºâ€œæ¨è + å»ºè®®â€
    """
    if not client:
        return 0, "API Key æœªé…ç½®ï¼Œé»˜è®¤æ¨èï¼š"
    
    if not candidates:
        return 0, "æ²¡æœ‰å€™é€‰èœè°±ã€‚"

    # 1. æ„å»ºå€™é€‰åˆ—è¡¨
    candidates_str = ""
    for i, doc in enumerate(candidates):
        snippet = doc.get('content', '')[:150].replace('\n', ' ')
        candidates_str += (
            f"é€‰é¡¹[{i}]: {doc.get('name')}\n"
            f"   - æ ‡ç­¾: {doc.get('tags', [])}\n"
            f"   - ç®€ä»‹: {snippet}...\n\n"
        )

    # =====================================================
    # âœ… ä¼˜åŒ–åçš„ Promptï¼šæ›´åƒä¸€ä¸ªæ‡‚å¾—å˜é€šçš„å¤§å¨
    # =====================================================
    system_prompt = """
    ä½ æ˜¯ä¸€ä½èªæ˜ã€æ‡‚å˜é€šçš„ç§å®¶å¤§å¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šçš„å€™é€‰èœè°±ä¸­ï¼Œä¸ºç”¨æˆ·æ¨è**æœ€åˆé€‚**çš„ä¸€é“ã€‚

    ã€æ¨èé€»è¾‘ã€‘ï¼š
    1. **æ‰¾æœ€å¤§å…¬çº¦æ•°**ï¼šä¼˜å…ˆé€‰æ‹©é£Ÿæã€å£å‘³æœ€æ¥è¿‘ç”¨æˆ·éœ€æ±‚çš„èœã€‚
    2. **çµæ´»å¤„ç†å¿Œå£**ï¼š
       - å¦‚æœç”¨æˆ·è¯´â€œä¸è¦è¾£â€ï¼Œå°½é‡é€‰ä¸è¾£çš„ã€‚
       - **å…³é”®ç‚¹**ï¼šå¦‚æœå€™é€‰é¡¹å…¨éƒ½æœ‰è¾£ï¼Œ**ä¸è¦æ‹’ç»å›ç­”ï¼** è¯·é€‰ä¸€ä¸ªæœ€å®¹æ˜“â€œå»è¾£â€çš„èœï¼ˆæ¯”å¦‚æŠŠè¾£æ¤’æ²¹æ¢æˆé¦™æ²¹ï¼‰ï¼Œå¹¶åœ¨ç†ç”±é‡Œå‘Šè¯‰ç”¨æˆ·æ€ä¹ˆè°ƒæ•´ã€‚
    3. **ä¸ä»…æ˜¯é€‰æ‹©ï¼Œæ›´æ˜¯å»ºè®®**ï¼šæ¨èç†ç”±è¦å‘Šè¯‰ç”¨æˆ·â€œä¸ºä»€ä¹ˆé€‰å®ƒâ€æˆ–è€…â€œæ€ä¹ˆåšæ›´ç¬¦åˆä½ çš„è¦æ±‚â€ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
    è¯·ç›´æ¥è¿”å›ä¸€è¡Œï¼šç´¢å¼•æ•°å­— ||| æ¨èç†ç”±
    ï¼ˆä¾‹å¦‚ï¼š1 ||| è™½ç„¶åŸè°±æœ‰è¾£æ¤’ï¼Œä½†è¿™é“èœåªè¦ä¸æ”¾è¾£æ¤’æ²¹ï¼Œä¾ç„¶éå¸¸é²œç¾ï¼Œå¾ˆé€‚åˆæ‚¨ã€‚ï¼‰
    """

    user_prompt = f"""
    ç”¨æˆ·éœ€æ±‚ï¼šã€{query}ã€‘

    å€™é€‰åˆ—è¡¨ï¼š
    {candidates_str}

    è¯·åšå‡ºä½ çš„é€‰æ‹©ï¼š
    """

    try:
        response = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.4, # ç¨å¾®æ”¾æ¾ä¸€ç‚¹åˆ›é€ åŠ›
            max_tokens=200
        )
        
        content = response.choices[0].message.content.strip()
        # print(f"ğŸ¤– [Generator] AI å»ºè®®: {content}") 

        # --- è§£æé€»è¾‘ (ä¿æŒé²æ£’æ€§) ---
        if "|||" in content:
            index_part, reason = content.split("|||", 1)
            match = re.search(r'\d+', index_part)
            if match:
                return int(match.group()), reason.strip()
        
        # å…œåº•ï¼šå¦‚æœ AI ç›´æ¥è¯´äº†æ•°å­—å¼€å¤´
        match = re.search(r'^\d+', content)
        if match:
             return int(match.group()), f"ä¸ºæ‚¨æ¨èã€{candidates[int(match.group())]['name']}ã€‘"

        # å½»åº•æ— æ³•è§£æ
        return 0, f"è¯•è¯•è¿™é“ã€{candidates[0]['name']}ã€‘ï¼Œåº”è¯¥ä¸é”™ï¼"

    except Exception as e:
        print(f"âŒ [Generator] æŠ¥é”™: {e}")
        return 0, "ä¸ºæ‚¨æ¨èä»¥ä¸‹èœè°±ï¼š"

